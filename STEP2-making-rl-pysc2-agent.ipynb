{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 2 - Making RL PySC2 Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Runnning 'Agent code' on jupyter notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfortunately, PySC2 uses Abseil, which treats python code as if its run like an app\n",
    "# This does not play well with jupyter notebook\n",
    "# So we will need to monkeypatch sys.argv\n",
    "\n",
    "\n",
    "import sys\n",
    "#sys.argv = [\"python\", \"--map\", \"AbyssalReef\"]\n",
    "sys.argv = [\"python\", \"--map\", \"Simple64\"]\n",
    "\n",
    "# Copyright 2017 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS-IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"Run an agent.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import importlib\n",
    "import threading\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from future.builtins import range  # pylint: disable=redefined-builtin\n",
    "\n",
    "from pysc2 import maps\n",
    "from pysc2.env import available_actions_printer\n",
    "from pysc2.env import run_loop\n",
    "from pysc2.env import sc2_env\n",
    "from pysc2.lib import point_flag\n",
    "from pysc2.lib import stopwatch\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "# because of Abseil's horrible design for running code underneath Colabs\n",
    "# We have to pull out this ugly hack from the hat\n",
    "if \"flags_defined\" not in globals():\n",
    "    flags.DEFINE_bool(\"render\", True, \"Whether to render with pygame.\")\n",
    "    point_flag.DEFINE_point(\"feature_screen_size\", \"84\",\n",
    "                            \"Resolution for screen feature layers.\")\n",
    "    point_flag.DEFINE_point(\"feature_minimap_size\", \"64\",\n",
    "                            \"Resolution for minimap feature layers.\")\n",
    "    point_flag.DEFINE_point(\"rgb_screen_size\", None,\n",
    "                            \"Resolution for rendered screen.\")\n",
    "    point_flag.DEFINE_point(\"rgb_minimap_size\", None,\n",
    "                            \"Resolution for rendered minimap.\")\n",
    "    flags.DEFINE_enum(\"action_space\", None, sc2_env.ActionSpace._member_names_,  # pylint: disable=protected-access\n",
    "                      \"Which action space to use. Needed if you take both feature \"\n",
    "                      \"and rgb observations.\")\n",
    "    flags.DEFINE_bool(\"use_feature_units\", True,\n",
    "                      \"Whether to include feature units.\")\n",
    "    flags.DEFINE_bool(\"disable_fog\", False, \"Whether to disable Fog of War.\")\n",
    "\n",
    "    flags.DEFINE_integer(\"max_agent_steps\", 0, \"Total agent steps.\")\n",
    "    flags.DEFINE_integer(\"game_steps_per_episode\", None, \"Game steps per episode.\")\n",
    "    flags.DEFINE_integer(\"max_episodes\", 0, \"Total episodes.\")\n",
    "    flags.DEFINE_integer(\"step_mul\", 8, \"Game steps per agent step.\")\n",
    "    flags.DEFINE_float(\"fps\", 22.4, \"Frames per second to run the game.\")\n",
    "\n",
    "    #flags.DEFINE_string(\"agent\", \"sc2.agent.BasicAgent.ZergBasicAgent\",\n",
    "    #                    \"Which agent to run, as a python path to an Agent class.\")\n",
    "    #flags.DEFINE_enum(\"agent_race\", \"zerg\", sc2_env.Race._member_names_,  # pylint: disable=protected-access\n",
    "    #                  \"Agent 1's race.\")\n",
    "    flags.DEFINE_string(\"agent\", \"TerranRLAgent\",\n",
    "                        \"Which agent to run, as a python path to an Agent class.\")\n",
    "    flags.DEFINE_enum(\"agent_race\", \"terran\", sc2_env.Race._member_names_,  # pylint: disable=protected-access\n",
    "                      \"Agent 1's race.\")\n",
    "\n",
    "    flags.DEFINE_string(\"agent2\", \"Bot\", \"Second agent, either Bot or agent class.\")\n",
    "    flags.DEFINE_enum(\"agent2_race\", \"random\", sc2_env.Race._member_names_,  # pylint: disable=protected-access\n",
    "                      \"Agent 2's race.\")\n",
    "    flags.DEFINE_enum(\"difficulty\", \"very_easy\", sc2_env.Difficulty._member_names_,  # pylint: disable=protected-access\n",
    "                      \"If agent2 is a built-in Bot, it's strength.\")\n",
    "\n",
    "    flags.DEFINE_bool(\"profile\", False, \"Whether to turn on code profiling.\")\n",
    "    flags.DEFINE_bool(\"trace\", False, \"Whether to trace the code execution.\")\n",
    "    flags.DEFINE_integer(\"parallel\", 1, \"How many instances to run in parallel.\")\n",
    "\n",
    "    flags.DEFINE_bool(\"save_replay\", True, \"Whether to save a replay at the end.\")\n",
    "\n",
    "    flags.DEFINE_string(\"map\", None, \"Name of a map to use.\")\n",
    "    flags.mark_flag_as_required(\"map\")\n",
    "\n",
    "flags_defined = True\n",
    "\n",
    "def run_thread(agent_classes, players, map_name, visualize):\n",
    "  \"\"\"Run one thread worth of the environment with agents.\"\"\"\n",
    "  with sc2_env.SC2Env(\n",
    "      map_name=map_name,\n",
    "      players=players,\n",
    "      agent_interface_format=sc2_env.parse_agent_interface_format(\n",
    "          feature_screen=FLAGS.feature_screen_size,\n",
    "          feature_minimap=FLAGS.feature_minimap_size,\n",
    "          rgb_screen=FLAGS.rgb_screen_size,\n",
    "          rgb_minimap=FLAGS.rgb_minimap_size,\n",
    "          action_space=FLAGS.action_space,\n",
    "          use_feature_units=FLAGS.use_feature_units),\n",
    "      step_mul=FLAGS.step_mul,\n",
    "      game_steps_per_episode=FLAGS.game_steps_per_episode,\n",
    "      disable_fog=FLAGS.disable_fog,\n",
    "      visualize=visualize) as env:\n",
    "    env = available_actions_printer.AvailableActionsPrinter(env)\n",
    "    agents = [agent_cls() for agent_cls in agent_classes]\n",
    "    run_loop.run_loop(agents, env, FLAGS.max_agent_steps, FLAGS.max_episodes)\n",
    "    if FLAGS.save_replay:\n",
    "      env.save_replay(agent_classes[0].__name__)\n",
    "\n",
    "def main(unused_argv):\n",
    "  \"\"\"Run an agent.\"\"\"\n",
    "  #stopwatch.sw.enabled = FLAGS.profile or FLAGS.trace\n",
    "  #stopwatch.sw.trace = FLAGS.trace\n",
    "\n",
    "  map_inst = maps.get(FLAGS.map)\n",
    "\n",
    "  agent_classes = []\n",
    "  players = []\n",
    "\n",
    "  #agent_module, agent_name = FLAGS.agent.rsplit(\".\", 1)\n",
    "  #agent_cls = getattr(importlib.import_module(agent_module), agent_name)\n",
    "  #agent_classes.append(agent_cls)\n",
    "  agent_classes.append(TerranRLAgent)\n",
    "  players.append(sc2_env.Agent(sc2_env.Race[FLAGS.agent_race]))\n",
    "\n",
    "  if map_inst.players >= 2:\n",
    "    if FLAGS.agent2 == \"Bot\":\n",
    "      players.append(sc2_env.Bot(sc2_env.Race[FLAGS.agent2_race],\n",
    "                                 sc2_env.Difficulty[FLAGS.difficulty]))\n",
    "    else:\n",
    "      agent_module, agent_name = FLAGS.agent2.rsplit(\".\", 1)\n",
    "      agent_cls = getattr(importlib.import_module(agent_module), agent_name)\n",
    "      agent_classes.append(agent_cls)\n",
    "      players.append(sc2_env.Agent(sc2_env.Race[FLAGS.agent2_race]))\n",
    "\n",
    "  threads = []\n",
    "  for _ in range(FLAGS.parallel - 1):\n",
    "    t = threading.Thread(target=run_thread,\n",
    "                         args=(agent_classes, players, FLAGS.map, False))\n",
    "    threads.append(t)\n",
    "    t.start()\n",
    "\n",
    "  run_thread(agent_classes, players, FLAGS.map, FLAGS.render)\n",
    "\n",
    "  for t in threads:\n",
    "    t.join()\n",
    "\n",
    "  if FLAGS.profile:\n",
    "    pass\n",
    "    #print(stopwatch.sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating a RL PySC2 Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from pysc2.agents import base_agent\n",
    "from pysc2.env import sc2_env\n",
    "from pysc2.lib import actions, features, units\n",
    "from absl import app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference from https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow\n",
    "class QLearningTable:\n",
    "    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):\n",
    "        self.actions = actions  # a list\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.epsilon = e_greedy\n",
    "        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        self.check_state_exist(observation)\n",
    "\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            # choose best action\n",
    "            # state_action = self.q_table.ix[observation, :]\n",
    "            state_action = self.q_table.loc[observation, :]\n",
    "\n",
    "            # some actions have the same value\n",
    "            state_action = state_action.reindex(np.random.permutation(state_action.index))\n",
    "\n",
    "            action = state_action.idxmax()\n",
    "        else:\n",
    "            # choose random action\n",
    "            action = np.random.choice(self.actions)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def learn(self, s, a, r, s_):\n",
    "        self.check_state_exist(s_)\n",
    "        self.check_state_exist(s)\n",
    "\n",
    "        # q_predict = self.q_table.ix[s, a]\n",
    "        q_predict = self.q_table.loc[s, a]\n",
    "        # q_target = r + self.gamma * self.q_table.ix[s_, :].max()\n",
    "        q_target = r + self.gamma * self.q_table.loc[s_, :].max()\n",
    "\n",
    "        # update\n",
    "        # self.q_table.ix[s, a] += self.lr * (q_target - q_predict)\n",
    "        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)\n",
    "\n",
    "    def check_state_exist(self, state):\n",
    "        if state not in self.q_table.index:\n",
    "            # append new state to q table\n",
    "            self.q_table = self.q_table.append(\n",
    "                pd.Series([0] * len(self.actions), index=self.q_table.columns, name=state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TerranRLAgent(base_agent.BaseAgent):\n",
    "    def __init__(self):\n",
    "        super(TerranRLAgent, self).__init__()\n",
    "\n",
    "    def transformLocation(self, x, x_distance, y, y_distance):\n",
    "        if not self.base_top_left:\n",
    "            return [x - x_distance, y - y_distance]\n",
    "        \n",
    "        return [x + x_distance, y + y_distance]\n",
    "        \n",
    "    def step(self, obs):\n",
    "        super(TerranRLAgent, self).step(obs)\n",
    "\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        if obs.first():\n",
    "            player_y, player_x = (obs.observation.feature_minimap.player_relative == features.PlayerRelative.SELF).nonzero()\n",
    "            self.base_top_left = 1 if player_y.any() and player_y.mean() <= 31 else 0\n",
    "\n",
    "        return actions.FUNCTIONS.no_op()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [run code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  app.run(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defining Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from pysc2.agents import base_agent\n",
    "from pysc2.env import sc2_env\n",
    "from pysc2.lib import actions, features, units\n",
    "from absl import app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION_DO_NOTHING = 'donothing'\n",
    "ACTION_SELECT_SCV = 'selectscv'\n",
    "ACTION_BUILD_SUPPLY_DEPOT = 'buildsupplydepot'\n",
    "ACTION_BUILD_BARRACKS = 'buildbarracks'\n",
    "ACTION_SELECT_BARRACKS = 'selectbarracks'\n",
    "ACTION_BUILD_MARINE = 'buildmarine'\n",
    "ACTION_SELECT_ARMY = 'selectarmy'\n",
    "ACTION_ATTACK = 'attack'\n",
    "\n",
    "smart_actions = [\n",
    "    ACTION_DO_NOTHING,\n",
    "    ACTION_SELECT_SCV,\n",
    "    ACTION_BUILD_SUPPLY_DEPOT,\n",
    "    ACTION_BUILD_BARRACKS,\n",
    "    ACTION_SELECT_BARRACKS,\n",
    "    ACTION_BUILD_MARINE,\n",
    "    ACTION_SELECT_ARMY,\n",
    "    ACTION_ATTACK,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference from https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow\n",
    "class QLearningTable:\n",
    "    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):\n",
    "        self.actions = actions  # a list\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.epsilon = e_greedy\n",
    "        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        self.check_state_exist(observation)\n",
    "\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            # choose best action\n",
    "            # state_action = self.q_table.ix[observation, :]\n",
    "            state_action = self.q_table.loc[observation, :]\n",
    "\n",
    "            # some actions have the same value\n",
    "            state_action = state_action.reindex(np.random.permutation(state_action.index))\n",
    "\n",
    "            action = state_action.idxmax()\n",
    "        else:\n",
    "            # choose random action\n",
    "            action = np.random.choice(self.actions)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def learn(self, s, a, r, s_):\n",
    "        self.check_state_exist(s_)\n",
    "        self.check_state_exist(s)\n",
    "\n",
    "        # q_predict = self.q_table.ix[s, a]\n",
    "        q_predict = self.q_table.loc[s, a]\n",
    "        # q_target = r + self.gamma * self.q_table.ix[s_, :].max()\n",
    "        q_target = r + self.gamma * self.q_table.loc[s_, :].max()\n",
    "\n",
    "        # update\n",
    "        # self.q_table.ix[s, a] += self.lr * (q_target - q_predict)\n",
    "        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)\n",
    "\n",
    "    def check_state_exist(self, state):\n",
    "        if state not in self.q_table.index:\n",
    "            # append new state to q table\n",
    "            self.q_table = self.q_table.append(\n",
    "                pd.Series([0] * len(self.actions), index=self.q_table.columns, name=state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TerranRLAgent(base_agent.BaseAgent):\n",
    "    def __init__(self):\n",
    "        super(TerranRLAgent, self).__init__()\n",
    "\n",
    "        self.base_top_left = None \n",
    "        self.qlearn = QLearningTable(actions=list(range(len(smart_actions))))\n",
    "\n",
    "    def transformLocation(self, x, x_distance, y, y_distance):\n",
    "        if not self.base_top_left:\n",
    "            return [x - x_distance, y - y_distance]\n",
    "        \n",
    "        return [x + x_distance, y + y_distance]\n",
    "\n",
    "    def getMeanLocation(self, unitList):\n",
    "        sum_x = 0\n",
    "        sum_y = 0\n",
    "        for unit in unitList:\n",
    "            sum_x += unit.x\n",
    "            sum_y += unit.y\n",
    "        mean_x = sum_x / len(unitList)\n",
    "        mean_y = sum_y / len(unitList)\n",
    "        \n",
    "        return [mean_x, mean_y]\n",
    "    \n",
    "    def unit_type_is_selected(self, obs, unit_type):\n",
    "        if (len(obs.observation.single_select) > 0 and\n",
    "            obs.observation.single_select[0].unit_type == unit_type):\n",
    "              return True\n",
    "\n",
    "        if (len(obs.observation.multi_select) > 0 and\n",
    "            obs.observation.multi_select[0].unit_type == unit_type):\n",
    "              return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def get_units_by_type(self, obs, unit_type):\n",
    "        return [unit for unit in obs.observation.feature_units\n",
    "                if unit.unit_type == unit_type]\n",
    "\n",
    "    def can_do(self, obs, action):\n",
    "        return action in obs.observation.available_actions\n",
    "    \n",
    "    def step(self, obs):\n",
    "        super(TerranRLAgent, self).step(obs)\n",
    "\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        if obs.first():\n",
    "            player_y, player_x = (obs.observation.feature_minimap.player_relative == features.PlayerRelative.SELF).nonzero()\n",
    "            self.base_top_left = 1 if player_y.any() and player_y.mean() <= 31 else 0\n",
    "\n",
    "        smart_action = smart_actions[random.randrange(0, len(smart_actions) - 1)]\n",
    "        \n",
    "        if smart_action == ACTION_DO_NOTHING:\n",
    "            return actions.FUNCTIONS.no_op()\n",
    "\n",
    "        elif smart_action == ACTION_SELECT_SCV:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.select_point.id):\n",
    "                scvs = self.get_units_by_type(obs, units.Terran.SCV)\n",
    "                if len(scvs) > 0:\n",
    "                    scv = random.choice(scvs)\n",
    "                    if scv.x >= 0 and scv.y >= 0:\n",
    "                        return actions.FUNCTIONS.select_point(\"select\", (scv.x,\n",
    "                                                                              scv.y))\n",
    "        \n",
    "        elif smart_action == ACTION_BUILD_SUPPLY_DEPOT:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.Build_SupplyDepot_screen.id):\n",
    "                ccs = self.get_units_by_type(obs, units.Terran.CommandCenter)\n",
    "                if len(ccs) > 0:\n",
    "                    mean_x, mean_y = self.getMeanLocation(ccs)\n",
    "                    target = self.transformLocation(int(mean_x), 0, int(mean_y), 20)\n",
    "\n",
    "                    return actions.FUNCTIONS.Build_SupplyDepot_screen(\"now\", target)        \n",
    "        \n",
    "        elif smart_action == ACTION_BUILD_BARRACKS:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.Build_Barracks_screen.id):\n",
    "                ccs = self.get_units_by_type(obs, units.Terran.CommandCenter)\n",
    "                if len(ccs) > 0:\n",
    "                    mean_x, mean_y = self.getMeanLocation(ccs)\n",
    "                    target = self.transformLocation(int(mean_x), 20, int(mean_y), 0)\n",
    "\n",
    "                    return actions.FUNCTIONS.Build_Barracks_screen(\"now\", target)\n",
    "   \n",
    "        elif smart_action == ACTION_SELECT_BARRACKS:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.select_point.id):\n",
    "                barracks = self.get_units_by_type(obs, units.Terran.Barracks)\n",
    "                if len(barracks) > 0:\n",
    "                    barrack = random.choice(barracks)\n",
    "                    if barrack.x >= 0 and barrack.y >= 0:\n",
    "                        return actions.FUNCTIONS.select_point(\"select\", (barrack.x,\n",
    "                                                                              barrack.y))\n",
    "        \n",
    "        elif smart_action == ACTION_BUILD_MARINE:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.Train_Marine_quick.id):\n",
    "                return actions.FUNCTIONS.Train_Marine_quick(\"queued\")\n",
    "        \n",
    "        elif smart_action == ACTION_SELECT_ARMY:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.select_army.id):\n",
    "                return actions.FUNCTIONS.select_army(\"select\")\n",
    "        \n",
    "        elif smart_action == ACTION_ATTACK:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.Attack_minimap.id):\n",
    "                if self.base_top_left:\n",
    "                    return actions.FUNCTIONS.Attack_minimap(\"now\", [39, 45])\n",
    "                else:\n",
    "                    return actions.FUNCTIONS.Attack_minimap(\"now\", [21, 24])\n",
    "            \n",
    "        return actions.FUNCTIONS.no_op()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [run code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  app.run(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Defining States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from pysc2.agents import base_agent\n",
    "from pysc2.env import sc2_env\n",
    "from pysc2.lib import actions, features, units\n",
    "from absl import app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION_DO_NOTHING = 'donothing'\n",
    "ACTION_SELECT_SCV = 'selectscv'\n",
    "ACTION_BUILD_SUPPLY_DEPOT = 'buildsupplydepot'\n",
    "ACTION_BUILD_BARRACKS = 'buildbarracks'\n",
    "ACTION_SELECT_BARRACKS = 'selectbarracks'\n",
    "ACTION_BUILD_MARINE = 'buildmarine'\n",
    "ACTION_SELECT_ARMY = 'selectarmy'\n",
    "ACTION_ATTACK = 'attack'\n",
    "\n",
    "smart_actions = [\n",
    "    ACTION_DO_NOTHING,\n",
    "    ACTION_SELECT_SCV,\n",
    "    ACTION_BUILD_SUPPLY_DEPOT,\n",
    "    ACTION_BUILD_BARRACKS,\n",
    "    ACTION_SELECT_BARRACKS,\n",
    "    ACTION_BUILD_MARINE,\n",
    "    ACTION_SELECT_ARMY,\n",
    "    ACTION_ATTACK,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference from https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow\n",
    "class QLearningTable:\n",
    "    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):\n",
    "        self.actions = actions  # a list\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.epsilon = e_greedy\n",
    "        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        self.check_state_exist(observation)\n",
    "\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            # choose best action\n",
    "            # state_action = self.q_table.ix[observation, :]\n",
    "            state_action = self.q_table.loc[observation, :]\n",
    "\n",
    "            # some actions have the same value\n",
    "            state_action = state_action.reindex(np.random.permutation(state_action.index))\n",
    "\n",
    "            action = state_action.idxmax()\n",
    "        else:\n",
    "            # choose random action\n",
    "            action = np.random.choice(self.actions)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def learn(self, s, a, r, s_):\n",
    "        self.check_state_exist(s_)\n",
    "        self.check_state_exist(s)\n",
    "\n",
    "        # q_predict = self.q_table.ix[s, a]\n",
    "        q_predict = self.q_table.loc[s, a]\n",
    "        # q_target = r + self.gamma * self.q_table.ix[s_, :].max()\n",
    "        q_target = r + self.gamma * self.q_table.loc[s_, :].max()\n",
    "\n",
    "        # update\n",
    "        # self.q_table.ix[s, a] += self.lr * (q_target - q_predict)\n",
    "        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)\n",
    "\n",
    "    def check_state_exist(self, state):\n",
    "        if state not in self.q_table.index:\n",
    "            # append new state to q table\n",
    "            self.q_table = self.q_table.append(\n",
    "                pd.Series([0] * len(self.actions), index=self.q_table.columns, name=state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TerranRLAgent(base_agent.BaseAgent):\n",
    "    def __init__(self):\n",
    "        super(TerranRLAgent, self).__init__()\n",
    "\n",
    "        self.base_top_left = None \n",
    "        self.qlearn = QLearningTable(actions=list(range(len(smart_actions))))\n",
    "\n",
    "    def transformLocation(self, x, x_distance, y, y_distance):\n",
    "        if not self.base_top_left:\n",
    "            return [x - x_distance, y - y_distance]\n",
    "        \n",
    "        return [x + x_distance, y + y_distance]\n",
    "\n",
    "    def getMeanLocation(self, unitList):\n",
    "        sum_x = 0\n",
    "        sum_y = 0\n",
    "        for unit in unitList:\n",
    "            sum_x += unit.x\n",
    "            sum_y += unit.y\n",
    "        mean_x = sum_x / len(unitList)\n",
    "        mean_y = sum_y / len(unitList)\n",
    "        \n",
    "        return [mean_x, mean_y]\n",
    "    \n",
    "    def unit_type_is_selected(self, obs, unit_type):\n",
    "        if (len(obs.observation.single_select) > 0 and\n",
    "            obs.observation.single_select[0].unit_type == unit_type):\n",
    "              return True\n",
    "\n",
    "        if (len(obs.observation.multi_select) > 0 and\n",
    "            obs.observation.multi_select[0].unit_type == unit_type):\n",
    "              return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def get_units_by_type(self, obs, unit_type):\n",
    "        return [unit for unit in obs.observation.feature_units\n",
    "                if unit.unit_type == unit_type]\n",
    "\n",
    "    def can_do(self, obs, action):\n",
    "        return action in obs.observation.available_actions\n",
    "    \n",
    "    def step(self, obs):\n",
    "        super(TerranRLAgent, self).step(obs)\n",
    "\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        if obs.first():\n",
    "            player_y, player_x = (obs.observation.feature_minimap.player_relative == features.PlayerRelative.SELF).nonzero()\n",
    "            self.base_top_left = 1 if player_y.any() and player_y.mean() <= 31 else 0\n",
    "\n",
    "        supply_depot_count = len(self.get_units_by_type(obs, units.Terran.SupplyDepot))\n",
    "\n",
    "        barracks_count = len(self.get_units_by_type(obs, units.Terran.Barracks))\n",
    "            \n",
    "        supply_limit = obs.observation.player.food_cap\n",
    "        army_supply = obs.observation.player.food_used\n",
    "        \n",
    "        current_state = [\n",
    "            supply_depot_count,\n",
    "            barracks_count,\n",
    "            supply_limit,\n",
    "            army_supply,\n",
    "        ]\n",
    "        \n",
    "        rl_action = self.qlearn.choose_action(str(current_state))\n",
    "        smart_action = smart_actions[rl_action]\n",
    "        \n",
    "        if smart_action == ACTION_DO_NOTHING:\n",
    "            return actions.FUNCTIONS.no_op()\n",
    "\n",
    "        elif smart_action == ACTION_SELECT_SCV:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.select_point.id):\n",
    "                scvs = self.get_units_by_type(obs, units.Terran.SCV)\n",
    "                if len(scvs) > 0:\n",
    "                    scv = random.choice(scvs)\n",
    "                    if scv.x >= 0 and scv.y >= 0:\n",
    "                        return actions.FUNCTIONS.select_point(\"select\", (scv.x,\n",
    "                                                                              scv.y))\n",
    "        \n",
    "        elif smart_action == ACTION_BUILD_SUPPLY_DEPOT:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.Build_SupplyDepot_screen.id):\n",
    "                ccs = self.get_units_by_type(obs, units.Terran.CommandCenter)\n",
    "                if len(ccs) > 0:\n",
    "                    mean_x, mean_y = self.getMeanLocation(ccs)\n",
    "                    target = self.transformLocation(int(mean_x), 0, int(mean_y), 20)\n",
    "\n",
    "                    return actions.FUNCTIONS.Build_SupplyDepot_screen(\"now\", target)        \n",
    "        \n",
    "        elif smart_action == ACTION_BUILD_BARRACKS:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.Build_Barracks_screen.id):\n",
    "                ccs = self.get_units_by_type(obs, units.Terran.CommandCenter)\n",
    "                if len(ccs) > 0:\n",
    "                    mean_x, mean_y = self.getMeanLocation(ccs)\n",
    "                    target = self.transformLocation(int(mean_x), 20, int(mean_y), 0)\n",
    "\n",
    "                    return actions.FUNCTIONS.Build_Barracks_screen(\"now\", target)\n",
    "    \n",
    "        elif smart_action == ACTION_SELECT_BARRACKS:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.select_point.id):\n",
    "                barracks = self.get_units_by_type(obs, units.Terran.Barracks)\n",
    "                if len(barracks) > 0:\n",
    "                    barrack = random.choice(barracks)\n",
    "                    if barrack.x >= 0 and barrack.y >= 0:\n",
    "                        return actions.FUNCTIONS.select_point(\"select\", (barrack.x,\n",
    "                                                                              barrack.y))\n",
    "        \n",
    "        elif smart_action == ACTION_BUILD_MARINE:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.Train_Marine_quick.id):\n",
    "                return actions.FUNCTIONS.Train_Marine_quick(\"queued\")\n",
    "        \n",
    "        elif smart_action == ACTION_SELECT_ARMY:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.select_army.id):\n",
    "                return actions.FUNCTIONS.select_army(\"select\")\n",
    "        \n",
    "        elif smart_action == ACTION_ATTACK:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.Attack_minimap.id):\n",
    "                if self.base_top_left:\n",
    "                    return actions.FUNCTIONS.Attack_minimap(\"now\", [39, 45])\n",
    "                else:\n",
    "                    return actions.FUNCTIONS.Attack_minimap(\"now\", [21, 24])\n",
    "            \n",
    "        return actions.FUNCTIONS.no_op()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [run code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  app.run(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Defining Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from pysc2.agents import base_agent\n",
    "from pysc2.env import sc2_env\n",
    "from pysc2.lib import actions, features, units\n",
    "from absl import app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION_DO_NOTHING = 'donothing'\n",
    "ACTION_SELECT_SCV = 'selectscv'\n",
    "ACTION_BUILD_SUPPLY_DEPOT = 'buildsupplydepot'\n",
    "ACTION_BUILD_BARRACKS = 'buildbarracks'\n",
    "ACTION_SELECT_BARRACKS = 'selectbarracks'\n",
    "ACTION_BUILD_MARINE = 'buildmarine'\n",
    "ACTION_SELECT_ARMY = 'selectarmy'\n",
    "ACTION_ATTACK = 'attack'\n",
    "\n",
    "smart_actions = [\n",
    "    ACTION_DO_NOTHING,\n",
    "    ACTION_SELECT_SCV,\n",
    "    ACTION_BUILD_SUPPLY_DEPOT,\n",
    "    ACTION_BUILD_BARRACKS,\n",
    "    ACTION_SELECT_BARRACKS,\n",
    "    ACTION_BUILD_MARINE,\n",
    "    ACTION_SELECT_ARMY,\n",
    "    ACTION_ATTACK,\n",
    "]\n",
    "\n",
    "KILL_UNIT_REWARD = 0.2\n",
    "KILL_BUILDING_REWARD = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference from https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow\n",
    "class QLearningTable:\n",
    "    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):\n",
    "        self.actions = actions  # a list\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.epsilon = e_greedy\n",
    "        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        self.check_state_exist(observation)\n",
    "\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            # choose best action\n",
    "            # state_action = self.q_table.ix[observation, :]\n",
    "            state_action = self.q_table.loc[observation, :]\n",
    "\n",
    "            # some actions have the same value\n",
    "            state_action = state_action.reindex(np.random.permutation(state_action.index))\n",
    "\n",
    "            action = state_action.idxmax()\n",
    "        else:\n",
    "            # choose random action\n",
    "            action = np.random.choice(self.actions)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def learn(self, s, a, r, s_):\n",
    "        self.check_state_exist(s_)\n",
    "        self.check_state_exist(s)\n",
    "\n",
    "        # q_predict = self.q_table.ix[s, a]\n",
    "        q_predict = self.q_table.loc[s, a]\n",
    "        # q_target = r + self.gamma * self.q_table.ix[s_, :].max()\n",
    "        q_target = r + self.gamma * self.q_table.loc[s_, :].max()\n",
    "\n",
    "        # update\n",
    "        # self.q_table.ix[s, a] += self.lr * (q_target - q_predict)\n",
    "        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)\n",
    "\n",
    "    def check_state_exist(self, state):\n",
    "        if state not in self.q_table.index:\n",
    "            # append new state to q table\n",
    "            self.q_table = self.q_table.append(\n",
    "                pd.Series([0] * len(self.actions), index=self.q_table.columns, name=state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TerranRLAgent(base_agent.BaseAgent):\n",
    "    def __init__(self):\n",
    "        super(TerranRLAgent, self).__init__()\n",
    "\n",
    "        self.base_top_left = None \n",
    "        self.qlearn = QLearningTable(actions=list(range(len(smart_actions))))\n",
    "        \n",
    "        self.previous_killed_unit_score = 0\n",
    "        self.previous_killed_building_score = 0\n",
    "\n",
    "    def transformLocation(self, x, x_distance, y, y_distance):\n",
    "        if not self.base_top_left:\n",
    "            return [x - x_distance, y - y_distance]\n",
    "        \n",
    "        return [x + x_distance, y + y_distance]\n",
    "\n",
    "    def getMeanLocation(self, unitList):\n",
    "        sum_x = 0\n",
    "        sum_y = 0\n",
    "        for unit in unitList:\n",
    "            sum_x += unit.x\n",
    "            sum_y += unit.y\n",
    "        mean_x = sum_x / len(unitList)\n",
    "        mean_y = sum_y / len(unitList)\n",
    "        \n",
    "        return [mean_x, mean_y]\n",
    "    \n",
    "    def unit_type_is_selected(self, obs, unit_type):\n",
    "        if (len(obs.observation.single_select) > 0 and\n",
    "            obs.observation.single_select[0].unit_type == unit_type):\n",
    "              return True\n",
    "\n",
    "        if (len(obs.observation.multi_select) > 0 and\n",
    "            obs.observation.multi_select[0].unit_type == unit_type):\n",
    "              return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def get_units_by_type(self, obs, unit_type):\n",
    "        return [unit for unit in obs.observation.feature_units\n",
    "                if unit.unit_type == unit_type]\n",
    "\n",
    "    def can_do(self, obs, action):\n",
    "        return action in obs.observation.available_actions\n",
    "    \n",
    "    def step(self, obs):\n",
    "        super(TerranRLAgent, self).step(obs)\n",
    "\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        if obs.first():\n",
    "            player_y, player_x = (obs.observation.feature_minimap.player_relative == features.PlayerRelative.SELF).nonzero()\n",
    "            self.base_top_left = 1 if player_y.any() and player_y.mean() <= 31 else 0\n",
    "\n",
    "        supply_depot_count = len(self.get_units_by_type(obs, units.Terran.SupplyDepot))\n",
    "\n",
    "        barracks_count = len(self.get_units_by_type(obs, units.Terran.Barracks))\n",
    "            \n",
    "        supply_limit = obs.observation.player.food_cap\n",
    "        army_supply = obs.observation.player.food_used\n",
    "        \n",
    "        killed_unit_score = obs.observation.score_cumulative.killed_value_units\n",
    "        killed_building_score = obs.observation.score_cumulative.killed_value_structures\n",
    "        \n",
    "        current_state = [\n",
    "            supply_depot_count,\n",
    "            barracks_count,\n",
    "            supply_limit,\n",
    "            army_supply,\n",
    "        ]\n",
    "        \n",
    "        reward = 0\n",
    "        \n",
    "        if killed_unit_score > self.previous_killed_unit_score:\n",
    "            reward += KILL_UNIT_REWARD\n",
    "                \n",
    "        if killed_building_score > self.previous_killed_building_score:\n",
    "            reward += KILL_BUILDING_REWARD\n",
    "                \n",
    "        rl_action = self.qlearn.choose_action(str(current_state))\n",
    "        smart_action = smart_actions[rl_action]\n",
    "        \n",
    "        self.previous_killed_unit_score = killed_unit_score\n",
    "        self.previous_killed_building_score = killed_building_score\n",
    "        \n",
    "        if smart_action == ACTION_DO_NOTHING:\n",
    "            return actions.FUNCTIONS.no_op()\n",
    "\n",
    "        elif smart_action == ACTION_SELECT_SCV:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.select_point.id):\n",
    "                scvs = self.get_units_by_type(obs, units.Terran.SCV)\n",
    "                if len(scvs) > 0:\n",
    "                    scv = random.choice(scvs)\n",
    "                    if scv.x >= 0 and scv.y >= 0:\n",
    "                        return actions.FUNCTIONS.select_point(\"select\", (scv.x,\n",
    "                                                                              scv.y))\n",
    "        \n",
    "        elif smart_action == ACTION_BUILD_SUPPLY_DEPOT:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.Build_SupplyDepot_screen.id):\n",
    "                ccs = self.get_units_by_type(obs, units.Terran.CommandCenter)\n",
    "                if len(ccs) > 0:\n",
    "                    mean_x, mean_y = self.getMeanLocation(ccs)\n",
    "                    target = self.transformLocation(int(mean_x), 0, int(mean_y), 20)\n",
    "\n",
    "                    return actions.FUNCTIONS.Build_SupplyDepot_screen(\"now\", target)        \n",
    "        \n",
    "        elif smart_action == ACTION_BUILD_BARRACKS:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.Build_Barracks_screen.id):\n",
    "                ccs = self.get_units_by_type(obs, units.Terran.CommandCenter)\n",
    "                if len(ccs) > 0:\n",
    "                    mean_x, mean_y = self.getMeanLocation(ccs)\n",
    "                    target = self.transformLocation(int(mean_x), 20, int(mean_y), 0)\n",
    "\n",
    "                    return actions.FUNCTIONS.Build_Barracks_screen(\"now\", target)\n",
    "    \n",
    "        elif smart_action == ACTION_SELECT_BARRACKS:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.select_point.id):\n",
    "                barracks = self.get_units_by_type(obs, units.Terran.Barracks)\n",
    "                if len(barracks) > 0:\n",
    "                    barrack = random.choice(barracks)\n",
    "                    if barrack.x >= 0 and barrack.y >= 0:\n",
    "                        return actions.FUNCTIONS.select_point(\"select\", (barrack.x,\n",
    "                                                                              barrack.y))\n",
    "        \n",
    "        elif smart_action == ACTION_BUILD_MARINE:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.Train_Marine_quick.id):\n",
    "                return actions.FUNCTIONS.Train_Marine_quick(\"queued\")\n",
    "        \n",
    "        elif smart_action == ACTION_SELECT_ARMY:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.select_army.id):\n",
    "                return actions.FUNCTIONS.select_army(\"select\")\n",
    "        \n",
    "        elif smart_action == ACTION_ATTACK:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.Attack_minimap.id):\n",
    "                if self.base_top_left:\n",
    "                    return actions.FUNCTIONS.Attack_minimap(\"now\", [39, 45])\n",
    "                else:\n",
    "                    return actions.FUNCTIONS.Attack_minimap(\"now\", [21, 24])\n",
    "            \n",
    "        return actions.FUNCTIONS.no_op()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [run code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  app.run(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Connecting All Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from pysc2.agents import base_agent\n",
    "from pysc2.env import sc2_env\n",
    "from pysc2.lib import actions, features, units\n",
    "from absl import app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION_DO_NOTHING = 'donothing'\n",
    "ACTION_SELECT_SCV = 'selectscv'\n",
    "ACTION_BUILD_SUPPLY_DEPOT = 'buildsupplydepot'\n",
    "ACTION_BUILD_BARRACKS = 'buildbarracks'\n",
    "ACTION_SELECT_BARRACKS = 'selectbarracks'\n",
    "ACTION_BUILD_MARINE = 'buildmarine'\n",
    "ACTION_SELECT_ARMY = 'selectarmy'\n",
    "ACTION_ATTACK = 'attack'\n",
    "\n",
    "smart_actions = [\n",
    "    ACTION_DO_NOTHING,\n",
    "    ACTION_SELECT_SCV,\n",
    "    ACTION_BUILD_SUPPLY_DEPOT,\n",
    "    ACTION_BUILD_BARRACKS,\n",
    "    ACTION_SELECT_BARRACKS,\n",
    "    ACTION_BUILD_MARINE,\n",
    "    ACTION_SELECT_ARMY,\n",
    "    ACTION_ATTACK,\n",
    "]\n",
    "\n",
    "KILL_UNIT_REWARD = 0.2\n",
    "KILL_BUILDING_REWARD = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference from https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow\n",
    "class QLearningTable:\n",
    "    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):\n",
    "        self.actions = actions  # a list\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.epsilon = e_greedy\n",
    "        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        self.check_state_exist(observation)\n",
    "\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            # choose best action\n",
    "            # state_action = self.q_table.ix[observation, :]\n",
    "            state_action = self.q_table.loc[observation, :]\n",
    "\n",
    "            # some actions have the same value\n",
    "            state_action = state_action.reindex(np.random.permutation(state_action.index))\n",
    "\n",
    "            action = state_action.idxmax()\n",
    "        else:\n",
    "            # choose random action\n",
    "            action = np.random.choice(self.actions)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def learn(self, s, a, r, s_):\n",
    "        self.check_state_exist(s_)\n",
    "        self.check_state_exist(s)\n",
    "\n",
    "        # q_predict = self.q_table.ix[s, a]\n",
    "        q_predict = self.q_table.loc[s, a]\n",
    "        # q_target = r + self.gamma * self.q_table.ix[s_, :].max()\n",
    "        q_target = r + self.gamma * self.q_table.loc[s_, :].max()\n",
    "\n",
    "        # update\n",
    "        # self.q_table.ix[s, a] += self.lr * (q_target - q_predict)\n",
    "        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)\n",
    "\n",
    "    def check_state_exist(self, state):\n",
    "        if state not in self.q_table.index:\n",
    "            # append new state to q table\n",
    "            self.q_table = self.q_table.append(\n",
    "                pd.Series([0] * len(self.actions), index=self.q_table.columns, name=state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TerranRLAgent(base_agent.BaseAgent):\n",
    "    def __init__(self):\n",
    "        super(TerranRLAgent, self).__init__()\n",
    "\n",
    "        self.base_top_left = None \n",
    "        self.qlearn = QLearningTable(actions=list(range(len(smart_actions))))\n",
    "        \n",
    "        self.previous_killed_unit_score = 0\n",
    "        self.previous_killed_building_score = 0\n",
    "        \n",
    "        self.previous_action = None\n",
    "        self.previous_state = None\n",
    "\n",
    "    def transformLocation(self, x, x_distance, y, y_distance):\n",
    "        if not self.base_top_left:\n",
    "            return [x - x_distance, y - y_distance]\n",
    "        \n",
    "        return [x + x_distance, y + y_distance]\n",
    "\n",
    "    def getMeanLocation(self, unitList):\n",
    "        sum_x = 0\n",
    "        sum_y = 0\n",
    "        for unit in unitList:\n",
    "            sum_x += unit.x\n",
    "            sum_y += unit.y\n",
    "        mean_x = sum_x / len(unitList)\n",
    "        mean_y = sum_y / len(unitList)\n",
    "        \n",
    "        return [mean_x, mean_y]\n",
    "    \n",
    "    def unit_type_is_selected(self, obs, unit_type):\n",
    "        if (len(obs.observation.single_select) > 0 and\n",
    "            obs.observation.single_select[0].unit_type == unit_type):\n",
    "              return True\n",
    "\n",
    "        if (len(obs.observation.multi_select) > 0 and\n",
    "            obs.observation.multi_select[0].unit_type == unit_type):\n",
    "              return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def get_units_by_type(self, obs, unit_type):\n",
    "        return [unit for unit in obs.observation.feature_units\n",
    "                if unit.unit_type == unit_type]\n",
    "\n",
    "    def can_do(self, obs, action):\n",
    "        return action in obs.observation.available_actions\n",
    "    \n",
    "    def step(self, obs):\n",
    "        super(TerranRLAgent, self).step(obs)\n",
    "\n",
    "        #time.sleep(0.5)\n",
    "        \n",
    "        if obs.first():\n",
    "            player_y, player_x = (obs.observation.feature_minimap.player_relative == features.PlayerRelative.SELF).nonzero()\n",
    "            self.base_top_left = 1 if player_y.any() and player_y.mean() <= 31 else 0\n",
    "\n",
    "        supply_depot_count = len(self.get_units_by_type(obs, units.Terran.SupplyDepot))\n",
    "\n",
    "        barracks_count = len(self.get_units_by_type(obs, units.Terran.Barracks))\n",
    "            \n",
    "        supply_limit = obs.observation.player.food_cap\n",
    "        army_supply = obs.observation.player.food_used\n",
    "        \n",
    "        killed_unit_score = obs.observation.score_cumulative.killed_value_units\n",
    "        killed_building_score = obs.observation.score_cumulative.killed_value_structures\n",
    "        \n",
    "        current_state = [\n",
    "            supply_depot_count,\n",
    "            barracks_count,\n",
    "            supply_limit,\n",
    "            army_supply,\n",
    "        ]\n",
    "        \n",
    "        if self.previous_action is not None:\n",
    "            reward = 0\n",
    "                \n",
    "            if killed_unit_score > self.previous_killed_unit_score:\n",
    "                reward += KILL_UNIT_REWARD\n",
    "                    \n",
    "            if killed_building_score > self.previous_killed_building_score:\n",
    "                reward += KILL_BUILDING_REWARD\n",
    "                \n",
    "            self.qlearn.learn(str(self.previous_state), self.previous_action, reward, str(current_state))\n",
    "        \n",
    "        rl_action = self.qlearn.choose_action(str(current_state))\n",
    "        smart_action = smart_actions[rl_action]\n",
    "        \n",
    "        self.previous_killed_unit_score = killed_unit_score\n",
    "        self.previous_killed_building_score = killed_building_score\n",
    "        self.previous_state = current_state\n",
    "        self.previous_action = rl_action\n",
    "        \n",
    "        if smart_action == ACTION_DO_NOTHING:\n",
    "            return actions.FUNCTIONS.no_op()\n",
    "\n",
    "        elif smart_action == ACTION_SELECT_SCV:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.select_point.id):\n",
    "                scvs = self.get_units_by_type(obs, units.Terran.SCV)\n",
    "                if len(scvs) > 0:\n",
    "                    scv = random.choice(scvs)\n",
    "                    if scv.x >= 0 and scv.y >= 0:\n",
    "                        return actions.FUNCTIONS.select_point(\"select\", (scv.x,\n",
    "                                                                              scv.y))\n",
    "        \n",
    "        elif smart_action == ACTION_BUILD_SUPPLY_DEPOT:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.Build_SupplyDepot_screen.id):\n",
    "                ccs = self.get_units_by_type(obs, units.Terran.CommandCenter)\n",
    "                if len(ccs) > 0:\n",
    "                    mean_x, mean_y = self.getMeanLocation(ccs)\n",
    "                    target = self.transformLocation(int(mean_x), 0, int(mean_y), 20)\n",
    "\n",
    "                    return actions.FUNCTIONS.Build_SupplyDepot_screen(\"now\", target)        \n",
    "        \n",
    "        elif smart_action == ACTION_BUILD_BARRACKS:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.Build_Barracks_screen.id):\n",
    "                ccs = self.get_units_by_type(obs, units.Terran.CommandCenter)\n",
    "                if len(ccs) > 0:\n",
    "                    mean_x, mean_y = self.getMeanLocation(ccs)\n",
    "                    target = self.transformLocation(int(mean_x), 20, int(mean_y), 0)\n",
    "\n",
    "                    return actions.FUNCTIONS.Build_Barracks_screen(\"now\", target)\n",
    "    \n",
    "        elif smart_action == ACTION_SELECT_BARRACKS:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.select_point.id):\n",
    "                barracks = self.get_units_by_type(obs, units.Terran.Barracks)\n",
    "                if len(barracks) > 0:\n",
    "                    barrack = random.choice(barracks)\n",
    "                    if barrack.x >= 0 and barrack.y >= 0:\n",
    "                        return actions.FUNCTIONS.select_point(\"select\", (barrack.x,\n",
    "                                                                              barrack.y))\n",
    "        \n",
    "        elif smart_action == ACTION_BUILD_MARINE:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.Train_Marine_quick.id):\n",
    "                return actions.FUNCTIONS.Train_Marine_quick(\"queued\")\n",
    "        \n",
    "        elif smart_action == ACTION_SELECT_ARMY:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.select_army.id):\n",
    "                return actions.FUNCTIONS.select_army(\"select\")\n",
    "        \n",
    "        elif smart_action == ACTION_ATTACK:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.Attack_minimap.id):\n",
    "                if self.base_top_left:\n",
    "                    return actions.FUNCTIONS.Attack_minimap(\"now\", [39, 45])\n",
    "                else:\n",
    "                    return actions.FUNCTIONS.Attack_minimap(\"now\", [21, 24])\n",
    "            \n",
    "        return actions.FUNCTIONS.no_op()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [run code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  app.run(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Changing Attack Actions\n",
    "\n",
    "![Reduced attack points in Simple64 map](./images/reduced_attack_points_in_Simple64.png)\n",
    "image ref : Steven Brown's PySC2 blog\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from pysc2.agents import base_agent\n",
    "from pysc2.env import sc2_env\n",
    "from pysc2.lib import actions, features, units\n",
    "from absl import app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION_DO_NOTHING = 'donothing'\n",
    "ACTION_SELECT_SCV = 'selectscv'\n",
    "ACTION_BUILD_SUPPLY_DEPOT = 'buildsupplydepot'\n",
    "ACTION_BUILD_BARRACKS = 'buildbarracks'\n",
    "ACTION_SELECT_BARRACKS = 'selectbarracks'\n",
    "ACTION_BUILD_MARINE = 'buildmarine'\n",
    "ACTION_SELECT_ARMY = 'selectarmy'\n",
    "ACTION_ATTACK = 'attack'\n",
    "\n",
    "smart_actions = [\n",
    "    ACTION_DO_NOTHING,\n",
    "    ACTION_SELECT_SCV,\n",
    "    ACTION_BUILD_SUPPLY_DEPOT,\n",
    "    ACTION_BUILD_BARRACKS,\n",
    "    ACTION_SELECT_BARRACKS,\n",
    "    ACTION_BUILD_MARINE,\n",
    "    ACTION_SELECT_ARMY,\n",
    "]\n",
    "\n",
    "#for mm_x in range(0, 64):\n",
    "#    for mm_y in range(0, 64):\n",
    "#        smart_actions.append(ACTION_ATTACK + '_' + str(mm_x) + '_' + str(mm_y))\n",
    "\n",
    "for mm_x in range(0, 64):\n",
    "    for mm_y in range(0, 64):\n",
    "        if (mm_x + 1) % 16 == 0 and (mm_y + 1) % 16 == 0:\n",
    "            smart_actions.append(ACTION_ATTACK + '_' + str(mm_x - 8) + '_' + str(mm_y - 8))\n",
    "\n",
    "KILL_UNIT_REWARD = 0.2\n",
    "KILL_BUILDING_REWARD = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference from https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow\n",
    "class QLearningTable:\n",
    "    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):\n",
    "        self.actions = actions  # a list\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.epsilon = e_greedy\n",
    "        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        self.check_state_exist(observation)\n",
    "\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            # choose best action\n",
    "            # state_action = self.q_table.ix[observation, :]\n",
    "            state_action = self.q_table.loc[observation, :]\n",
    "\n",
    "            # some actions have the same value\n",
    "            state_action = state_action.reindex(np.random.permutation(state_action.index))\n",
    "\n",
    "            action = state_action.idxmax()\n",
    "        else:\n",
    "            # choose random action\n",
    "            action = np.random.choice(self.actions)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def learn(self, s, a, r, s_):\n",
    "        self.check_state_exist(s_)\n",
    "        self.check_state_exist(s)\n",
    "\n",
    "        # q_predict = self.q_table.ix[s, a]\n",
    "        q_predict = self.q_table.loc[s, a]\n",
    "        # q_target = r + self.gamma * self.q_table.ix[s_, :].max()\n",
    "        q_target = r + self.gamma * self.q_table.loc[s_, :].max()\n",
    "\n",
    "        # update\n",
    "        # self.q_table.ix[s, a] += self.lr * (q_target - q_predict)\n",
    "        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)\n",
    "\n",
    "    def check_state_exist(self, state):\n",
    "        if state not in self.q_table.index:\n",
    "            # append new state to q table\n",
    "            self.q_table = self.q_table.append(\n",
    "                pd.Series([0] * len(self.actions), index=self.q_table.columns, name=state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TerranRLAgent(base_agent.BaseAgent):\n",
    "    def __init__(self):\n",
    "        super(TerranRLAgent, self).__init__()\n",
    "\n",
    "        self.base_top_left = None \n",
    "        self.qlearn = QLearningTable(actions=list(range(len(smart_actions))))\n",
    "        \n",
    "        self.previous_killed_unit_score = 0\n",
    "        self.previous_killed_building_score = 0\n",
    "        \n",
    "        self.previous_action = None\n",
    "        self.previous_state = None\n",
    "\n",
    "    def transformDistance(self, x, x_distance, y, y_distance):\n",
    "        if not self.base_top_left:\n",
    "            return [x - x_distance, y - y_distance]\n",
    "        \n",
    "        return [x + x_distance, y + y_distance]\n",
    "    \n",
    "    def transformLocation(self, x, y):\n",
    "        if not self.base_top_left:\n",
    "            return [64 - x, 64 - y]\n",
    "        \n",
    "        return [x, y]\n",
    "\n",
    "    def getMeanLocation(self, unitList):\n",
    "        sum_x = 0\n",
    "        sum_y = 0\n",
    "        for unit in unitList:\n",
    "            sum_x += unit.x\n",
    "            sum_y += unit.y\n",
    "        mean_x = sum_x / len(unitList)\n",
    "        mean_y = sum_y / len(unitList)\n",
    "        \n",
    "        return [mean_x, mean_y]\n",
    "    \n",
    "    def unit_type_is_selected(self, obs, unit_type):\n",
    "        if (len(obs.observation.single_select) > 0 and\n",
    "            obs.observation.single_select[0].unit_type == unit_type):\n",
    "              return True\n",
    "\n",
    "        if (len(obs.observation.multi_select) > 0 and\n",
    "            obs.observation.multi_select[0].unit_type == unit_type):\n",
    "              return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def get_units_by_type(self, obs, unit_type):\n",
    "        return [unit for unit in obs.observation.feature_units\n",
    "                if unit.unit_type == unit_type]\n",
    "\n",
    "    def can_do(self, obs, action):\n",
    "        return action in obs.observation.available_actions\n",
    "    \n",
    "    def step(self, obs):\n",
    "        super(TerranRLAgent, self).step(obs)\n",
    "\n",
    "        #time.sleep(0.5)\n",
    "        \n",
    "        if obs.first():\n",
    "            player_y, player_x = (obs.observation.feature_minimap.player_relative == features.PlayerRelative.SELF).nonzero()\n",
    "            self.base_top_left = 1 if player_y.any() and player_y.mean() <= 31 else 0\n",
    "\n",
    "        supply_depot_count = len(self.get_units_by_type(obs, units.Terran.SupplyDepot))\n",
    "\n",
    "        barracks_count = len(self.get_units_by_type(obs, units.Terran.Barracks))\n",
    "            \n",
    "        supply_limit = obs.observation.player.food_cap\n",
    "        army_supply = obs.observation.player.food_used\n",
    "        \n",
    "        killed_unit_score = obs.observation.score_cumulative.killed_value_units\n",
    "        killed_building_score = obs.observation.score_cumulative.killed_value_structures\n",
    "        \n",
    "        current_state = [\n",
    "            supply_depot_count,\n",
    "            barracks_count,\n",
    "            supply_limit,\n",
    "            army_supply,\n",
    "        ]\n",
    "        \n",
    "        if self.previous_action is not None:\n",
    "            reward = 0\n",
    "                \n",
    "            if killed_unit_score > self.previous_killed_unit_score:\n",
    "                reward += KILL_UNIT_REWARD\n",
    "                    \n",
    "            if killed_building_score > self.previous_killed_building_score:\n",
    "                reward += KILL_BUILDING_REWARD\n",
    "                \n",
    "            self.qlearn.learn(str(self.previous_state), self.previous_action, reward, str(current_state))\n",
    "        \n",
    "        rl_action = self.qlearn.choose_action(str(current_state))\n",
    "        smart_action = smart_actions[rl_action]\n",
    "        \n",
    "        self.previous_killed_unit_score = killed_unit_score\n",
    "        self.previous_killed_building_score = killed_building_score\n",
    "        self.previous_state = current_state\n",
    "        self.previous_action = rl_action\n",
    "        \n",
    "        x = 0\n",
    "        y = 0\n",
    "        if '_' in smart_action:\n",
    "            smart_action, x, y = smart_action.split('_')\n",
    "        \n",
    "        if smart_action == ACTION_DO_NOTHING:\n",
    "            return actions.FUNCTIONS.no_op()\n",
    "\n",
    "        elif smart_action == ACTION_SELECT_SCV:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.select_point.id):\n",
    "                scvs = self.get_units_by_type(obs, units.Terran.SCV)\n",
    "                if len(scvs) > 0:\n",
    "                    scv = random.choice(scvs)\n",
    "                    if scv.x >= 0 and scv.y >= 0:\n",
    "                        return actions.FUNCTIONS.select_point(\"select\", (scv.x,\n",
    "                                                                              scv.y))\n",
    "        \n",
    "        elif smart_action == ACTION_BUILD_SUPPLY_DEPOT:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.Build_SupplyDepot_screen.id):\n",
    "                ccs = self.get_units_by_type(obs, units.Terran.CommandCenter)\n",
    "                if len(ccs) > 0:\n",
    "                    mean_x, mean_y = self.getMeanLocation(ccs)\n",
    "                    target = self.transformDistance(int(mean_x), 0, int(mean_y), 20)\n",
    "\n",
    "                    return actions.FUNCTIONS.Build_SupplyDepot_screen(\"now\", target)        \n",
    "        \n",
    "        elif smart_action == ACTION_BUILD_BARRACKS:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.Build_Barracks_screen.id):\n",
    "                ccs = self.get_units_by_type(obs, units.Terran.CommandCenter)\n",
    "                if len(ccs) > 0:\n",
    "                    mean_x, mean_y = self.getMeanLocation(ccs)\n",
    "                    target = self.transformDistance(int(mean_x), 20, int(mean_y), 0)\n",
    "\n",
    "                    return actions.FUNCTIONS.Build_Barracks_screen(\"now\", target)\n",
    "    \n",
    "        elif smart_action == ACTION_SELECT_BARRACKS:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.select_point.id):\n",
    "                barracks = self.get_units_by_type(obs, units.Terran.Barracks)\n",
    "                if len(barracks) > 0:\n",
    "                    barrack = random.choice(barracks)\n",
    "                    if barrack.x >= 0 and barrack.y >= 0:\n",
    "                        return actions.FUNCTIONS.select_point(\"select\", (barrack.x,\n",
    "                                                                              barrack.y))\n",
    "        \n",
    "        elif smart_action == ACTION_BUILD_MARINE:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.Train_Marine_quick.id):\n",
    "                return actions.FUNCTIONS.Train_Marine_quick(\"queued\")\n",
    "        \n",
    "        elif smart_action == ACTION_SELECT_ARMY:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.select_army.id):\n",
    "                return actions.FUNCTIONS.select_army(\"select\")\n",
    "        \n",
    "        elif smart_action == ACTION_ATTACK:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.Attack_minimap.id):\n",
    "                return actions.FUNCTIONS.Attack_minimap(\"now\", self.transformLocation(int(x), int(y)))\n",
    "            \n",
    "        return actions.FUNCTIONS.no_op()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [run code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  app.run(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Adding Enemy Position to States\n",
    "\n",
    "![Reduced enemy position grid in Simple64 map](./images/reduced_enemy_position_grid_in_Simple64.png)\n",
    "image ref : Steven Brown's PySC2 blog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import math\n",
    "import os.path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from pysc2.agents import base_agent\n",
    "from pysc2.env import sc2_env\n",
    "from pysc2.lib import actions, features, units\n",
    "from absl import app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = 'rlagent_learning_data'\n",
    "\n",
    "ACTION_DO_NOTHING = 'donothing'\n",
    "ACTION_SELECT_SCV = 'selectscv'\n",
    "ACTION_BUILD_SUPPLY_DEPOT = 'buildsupplydepot'\n",
    "ACTION_BUILD_BARRACKS = 'buildbarracks'\n",
    "ACTION_SELECT_BARRACKS = 'selectbarracks'\n",
    "ACTION_BUILD_MARINE = 'buildmarine'\n",
    "ACTION_SELECT_ARMY = 'selectarmy'\n",
    "ACTION_ATTACK = 'attack'\n",
    "\n",
    "smart_actions = [\n",
    "    ACTION_DO_NOTHING,\n",
    "    ACTION_SELECT_SCV,\n",
    "    ACTION_BUILD_SUPPLY_DEPOT,\n",
    "    ACTION_BUILD_BARRACKS,\n",
    "    ACTION_SELECT_BARRACKS,\n",
    "    ACTION_BUILD_MARINE,\n",
    "    ACTION_SELECT_ARMY,\n",
    "]\n",
    "\n",
    "#for mm_x in range(0, 64):\n",
    "#    for mm_y in range(0, 64):\n",
    "#        smart_actions.append(ACTION_ATTACK + '_' + str(mm_x) + '_' + str(mm_y))\n",
    "\n",
    "for mm_x in range(0, 64):\n",
    "    for mm_y in range(0, 64):\n",
    "        if (mm_x + 1) % 16 == 0 and (mm_y + 1) % 16 == 0:\n",
    "            smart_actions.append(ACTION_ATTACK + '_' + str(mm_x - 8) + '_' + str(mm_y - 8))\n",
    "\n",
    "KILL_UNIT_REWARD = 0.2\n",
    "KILL_BUILDING_REWARD = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference from https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow\n",
    "class QLearningTable:\n",
    "    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):\n",
    "        self.actions = actions  # a list\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.epsilon = e_greedy\n",
    "        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        self.check_state_exist(observation)\n",
    "\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            # choose best action\n",
    "            # state_action = self.q_table.ix[observation, :]\n",
    "            state_action = self.q_table.loc[observation, :]\n",
    "\n",
    "            # some actions have the same value\n",
    "            state_action = state_action.reindex(np.random.permutation(state_action.index))\n",
    "\n",
    "            action = state_action.idxmax()\n",
    "        else:\n",
    "            # choose random action\n",
    "            action = np.random.choice(self.actions)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def learn(self, s, a, r, s_):\n",
    "        self.check_state_exist(s_)\n",
    "        self.check_state_exist(s)\n",
    "\n",
    "        # q_predict = self.q_table.ix[s, a]\n",
    "        q_predict = self.q_table.loc[s, a]\n",
    "        # q_target = r + self.gamma * self.q_table.ix[s_, :].max()\n",
    "        q_target = r + self.gamma * self.q_table.loc[s_, :].max()\n",
    "\n",
    "        # update\n",
    "        # self.q_table.ix[s, a] += self.lr * (q_target - q_predict)\n",
    "        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)\n",
    "\n",
    "    def check_state_exist(self, state):\n",
    "        if state not in self.q_table.index:\n",
    "            # append new state to q table\n",
    "            self.q_table = self.q_table.append(\n",
    "                pd.Series([0] * len(self.actions), index=self.q_table.columns, name=state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TerranRLAgent(base_agent.BaseAgent):\n",
    "    def __init__(self):\n",
    "        super(TerranRLAgent, self).__init__()\n",
    "\n",
    "        self.base_top_left = None \n",
    "        self.qlearn = QLearningTable(actions=list(range(len(smart_actions))))\n",
    "        \n",
    "        self.previous_killed_unit_score = 0\n",
    "        self.previous_killed_building_score = 0\n",
    "        \n",
    "        self.previous_action = None\n",
    "        self.previous_state = None\n",
    "        \n",
    "        if os.path.isfile(DATA_FILE + '.gz'):\n",
    "            self.qlearn.q_table = pd.read_pickle(DATA_FILE + '.gz', compression='gzip')\n",
    "\n",
    "    def transformDistance(self, x, x_distance, y, y_distance):\n",
    "        if not self.base_top_left:\n",
    "            return [x - x_distance, y - y_distance]\n",
    "        \n",
    "        return [x + x_distance, y + y_distance]\n",
    "    \n",
    "    def transformLocation(self, x, y):\n",
    "        if not self.base_top_left:\n",
    "            return [64 - x, 64 - y]\n",
    "        \n",
    "        return [x, y]\n",
    "    \n",
    "    def getMeanLocation(self, unitList):\n",
    "        sum_x = 0\n",
    "        sum_y = 0\n",
    "        for unit in unitList:\n",
    "            sum_x += unit.x\n",
    "            sum_y += unit.y\n",
    "        mean_x = sum_x / len(unitList)\n",
    "        mean_y = sum_y / len(unitList)\n",
    "        \n",
    "        return [mean_x, mean_y]\n",
    "\n",
    "    def unit_type_is_selected(self, obs, unit_type):\n",
    "        if (len(obs.observation.single_select) > 0 and\n",
    "            obs.observation.single_select[0].unit_type == unit_type):\n",
    "              return True\n",
    "\n",
    "        if (len(obs.observation.multi_select) > 0 and\n",
    "            obs.observation.multi_select[0].unit_type == unit_type):\n",
    "              return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def get_units_by_type(self, obs, unit_type):\n",
    "        return [unit for unit in obs.observation.feature_units\n",
    "                if unit.unit_type == unit_type]\n",
    "\n",
    "    def can_do(self, obs, action):\n",
    "        return action in obs.observation.available_actions\n",
    "    \n",
    "    def step(self, obs):\n",
    "        super(TerranRLAgent, self).step(obs)\n",
    "\n",
    "        #time.sleep(0.5)\n",
    "        \n",
    "        if obs.last():\n",
    "            self.qlearn.q_table.to_pickle(DATA_FILE + '.gz', 'gzip')\n",
    "        \n",
    "        if obs.first():\n",
    "            player_y, player_x = (obs.observation.feature_minimap.player_relative == features.PlayerRelative.SELF).nonzero()\n",
    "            self.base_top_left = 1 if player_y.any() and player_y.mean() <= 31 else 0\n",
    "\n",
    "        supply_depot_count = len(self.get_units_by_type(obs, units.Terran.SupplyDepot))\n",
    "\n",
    "        barracks_count = len(self.get_units_by_type(obs, units.Terran.Barracks))\n",
    "            \n",
    "        supply_limit = obs.observation.player.food_cap\n",
    "        army_supply = obs.observation.player.food_used\n",
    "        \n",
    "        killed_unit_score = obs.observation.score_cumulative.killed_value_units\n",
    "        killed_building_score = obs.observation.score_cumulative.killed_value_structures\n",
    "        \n",
    "#        current_state = np.zeros(5000)\n",
    "#        current_state[0] = supply_depot_count\n",
    "#        current_state[1] = barracks_count\n",
    "#        current_state[2] = supply_limit\n",
    "#        current_state[3] = army_supply\n",
    "#\n",
    "#        hot_squares = np.zeros(4096)        \n",
    "#        enemy_y, enemy_x = (obs.observation.feature_minimap.player_relative == features.PlayerRelative.ENEMY).nonzero()\n",
    "#        for i in range(0, len(enemy_y)):\n",
    "#            y = int(enemy_y[i])\n",
    "#            x = int(enemy_x[i])\n",
    "#            \n",
    "#            hot_squares[((y - 1) * 64) + (x - 1)] = 1\n",
    "#        \n",
    "#        if not self.base_top_left:\n",
    "#            hot_squares = hot_squares[::-1]\n",
    "#        \n",
    "#        for i in range(0, 4096):\n",
    "#            current_state[i + 4] = hot_squares[i]\n",
    "\n",
    "        current_state = np.zeros(20)\n",
    "        current_state[0] = supply_depot_count\n",
    "        current_state[1] = barracks_count\n",
    "        current_state[2] = supply_limit\n",
    "        current_state[3] = army_supply\n",
    "\n",
    "        hot_squares = np.zeros(16)        \n",
    "        enemy_y, enemy_x = (obs.observation.feature_minimap.player_relative == features.PlayerRelative.ENEMY).nonzero()\n",
    "        for i in range(0, len(enemy_y)):\n",
    "            y = int(math.ceil((enemy_y[i] + 1) / 16))\n",
    "            x = int(math.ceil((enemy_x[i] + 1) / 16))\n",
    "            \n",
    "            hot_squares[((y - 1) * 4) + (x - 1)] = 1\n",
    "        \n",
    "        if not self.base_top_left:\n",
    "            hot_squares = hot_squares[::-1]\n",
    "        \n",
    "        for i in range(0, 16):\n",
    "            current_state[i + 4] = hot_squares[i] \n",
    "            \n",
    "        if self.previous_action is not None:\n",
    "            reward = 0\n",
    "                \n",
    "            if killed_unit_score > self.previous_killed_unit_score:\n",
    "                reward += KILL_UNIT_REWARD\n",
    "                    \n",
    "            if killed_building_score > self.previous_killed_building_score:\n",
    "                reward += KILL_BUILDING_REWARD\n",
    "                \n",
    "            self.qlearn.learn(str(self.previous_state), self.previous_action, reward, str(current_state))\n",
    "        \n",
    "        rl_action = self.qlearn.choose_action(str(current_state))\n",
    "        smart_action = smart_actions[rl_action]\n",
    "        \n",
    "        self.previous_killed_unit_score = killed_unit_score\n",
    "        self.previous_killed_building_score = killed_building_score\n",
    "        self.previous_state = current_state\n",
    "        self.previous_action = rl_action\n",
    "        \n",
    "        x = 0\n",
    "        y = 0\n",
    "        if '_' in smart_action:\n",
    "            smart_action, x, y = smart_action.split('_')\n",
    "        \n",
    "        if smart_action == ACTION_DO_NOTHING:\n",
    "            return actions.FUNCTIONS.no_op()\n",
    "\n",
    "        elif smart_action == ACTION_SELECT_SCV:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.select_point.id):\n",
    "                scvs = self.get_units_by_type(obs, units.Terran.SCV)\n",
    "                if len(scvs) > 0:\n",
    "                    scv = random.choice(scvs)\n",
    "                    if scv.x >= 0 and scv.y >= 0:\n",
    "                        return actions.FUNCTIONS.select_point(\"select\", (scv.x,\n",
    "                                                                              scv.y))\n",
    "        \n",
    "        elif smart_action == ACTION_BUILD_SUPPLY_DEPOT:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.Build_SupplyDepot_screen.id):\n",
    "                ccs = self.get_units_by_type(obs, units.Terran.CommandCenter)\n",
    "                if len(ccs) > 0:\n",
    "                    mean_x, mean_y = self.getMeanLocation(ccs)\n",
    "                    target = self.transformDistance(int(mean_x), 0, int(mean_y), 20)\n",
    "\n",
    "                    return actions.FUNCTIONS.Build_SupplyDepot_screen(\"now\", target)        \n",
    "        \n",
    "        elif smart_action == ACTION_BUILD_BARRACKS:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.Build_Barracks_screen.id):\n",
    "                ccs = self.get_units_by_type(obs, units.Terran.CommandCenter)\n",
    "                if len(ccs) > 0:\n",
    "                    mean_x, mean_y = self.getMeanLocation(ccs)\n",
    "                    target = self.transformDistance(int(mean_x), 20, int(mean_y), 0)\n",
    "\n",
    "                    return actions.FUNCTIONS.Build_Barracks_screen(\"now\", target)\n",
    "    \n",
    "        elif smart_action == ACTION_SELECT_BARRACKS:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.select_point.id):\n",
    "                barracks = self.get_units_by_type(obs, units.Terran.Barracks)\n",
    "                if len(barracks) > 0:\n",
    "                    barrack = random.choice(barracks)\n",
    "                    if barrack.x >= 0 and barrack.y >= 0:\n",
    "                        return actions.FUNCTIONS.select_point(\"select\", (barrack.x,\n",
    "                                                                              barrack.y))\n",
    "        \n",
    "        elif smart_action == ACTION_BUILD_MARINE:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.Train_Marine_quick.id):\n",
    "                return actions.FUNCTIONS.Train_Marine_quick(\"queued\")\n",
    "        \n",
    "        elif smart_action == ACTION_SELECT_ARMY:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.select_army.id):\n",
    "                return actions.FUNCTIONS.select_army(\"select\")\n",
    "        \n",
    "        elif smart_action == ACTION_ATTACK:\n",
    "            #if self.can_do(obs, actions.FUNCTIONS.Attack_minimap.id):\n",
    "            if not self.unit_type_is_selected(obs, units.Terran.SCV) and self.can_do(obs, actions.FUNCTIONS.Attack_minimap.id):\n",
    "                return actions.FUNCTIONS.Attack_minimap(\"now\", self.transformLocation(int(x), int(y)))\n",
    "            \n",
    "        return actions.FUNCTIONS.no_op()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [run code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  app.run(main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "starcraft2",
   "language": "python",
   "name": "starcraft2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
